# model configs.
model:
  type: "Attention"
  Attention:
    src_dim: 512
    hidden_dim: 128
    embedding_dim: 128
    layer_num: 1
    cell_type: "LSTM"
loss:
  type: "CrossEntropyLoss"
optimizer:
  type: "Adam"
  lr: 5e-4
  weight_decay: 1e-4
scheduler:
  type: "CircularLRBeta"
  lr_max: 5e-4
  lr_divider: 10
  cut_point: 10
  step_size: 834  # step size
  momentum:
    - 0.95
    - 0.85

# data configs.
data:
  train:
    path:
      - "/opt/ml/input/data/train_dataset/gt.txt"
    transforms:
      - "Resize(height=128, width=128)"
      - "Normalize(always_apply=True)"
  valid:
    path:
      - ""
    transforms:
      - null
  test:
    path:
      - ""
    transforms:
      - null
  token_paths:
    - "/opt/ml/input/data/train_dataset/tokens.txt"
  dataset_proportions:  # proportion of data to take from train (not test)
    - 1.0
  random_split: True  # if set True, divided to train and validation dataset from train path. valid args would be ignored.
  test_proportions: 0.2  # factor how many to take from train data for validation.
  crop: True
  rgb: True  # True for RGB, False for grayscale

# training configs.
train_config:
  batch_size: 96
  num_workers: 8
  num_epochs: 50
  print_interval: 1
  teacher_forcing_ratio: 0.5
  max_grad_norm: 2.0
seed: 42
wandb:
  project: "hangjoo"
  entity: "unnamed"
  name: "Attention_baseline"
prefix: "./log/attention_50"
checkpoint: ""