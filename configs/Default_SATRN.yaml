model:
  type: "Baseline_SATRN"
  encoder:
    hidden_dim: 300
    filter_dim: 600
    layer_num: 6
    head_num: 8
  decoder:
    src_dim: 300
    hidden_dim: 128
    filter_dim: 512
    layer_num: 3
    head_num: 8
  dropout_rate: 0.1
loss:
  type: "CrossEntropyLoss"
optimizer:
  type: "Adam"
  lr: 5e-4
  weight_decay: 1e-4
scheduler:
  type: "CircularLRBeta"
  lr_max: 5e-4
  lr_divider: 10
  cut_point: 10
  step_size: 834  # step size
  momentum:
    - 0.95
    - 0.85

data:
  train:
    path:
      - "/opt/ml/input/data/train_dataset/gt.txt"
    transforms:
      - "Resize(height=32, width=100)"
      - "Normalize(always_apply=True)"
  valid:
    path:
      - ""
    transforms:
      - null
  test:
    path:
      - ""
    transforms:
      - null
  token_paths:
    - "/opt/ml/input/data/train_dataset/tokens.txt"
  dataset_proportions:  # proportion of data to take from train (not test)
    - 1.0
  random_split: True  # if set True, divided to train and validation dataset from train path. valid args would be ignored.
  test_proportions: 0.2  # factor how many to take from train data for validation.
  rgb: True  # True for RGB, False for grayscale

train_config:
  batch_size: 64
  num_workers: 8
  num_epochs: 100
  print_interval: 1
  max_seq_len: 75  # if set an integer, data that has a sentence length over max_seq_len would be ignored.
  teacher_forcing_ratio: 0.5
  max_grad_norm: 2.0
  fp_16: True
seed: 42
wandb:
  project: "unnamed"
  entity: "unnamed"
  name: "SATRN_baseline_fp16"
prefix: "./log/SATRN"
checkpoint: ""